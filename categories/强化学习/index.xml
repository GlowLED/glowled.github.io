<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>强化学习 on GlowLED的后花园</title><link>https://blog.glowled.top/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/</link><description>Recent content from GlowLED的后花园</description><generator>Hugo</generator><language>zh-CN</language><managingEditor>zpeiyu11@gamil.com (GlowLED)</managingEditor><webMaster>zpeiyu11@gamil.com (GlowLED)</webMaster><copyright>本博客所有文章除特别声明外，均采用 BY-NC-SA 许可协议。转载请注明出处！</copyright><lastBuildDate>Tue, 03 Feb 2026 15:12:14 +0800</lastBuildDate><atom:link href="https://blog.glowled.top/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml"/><item><title>策略梯度定理</title><link>https://blog.glowled.top/post/policy-gradient/</link><pubDate>Tue, 03 Feb 2026 15:12:14 +0800</pubDate><author>zpeiyu11@gamil.com (GlowLED)</author><guid>https://blog.glowled.top/post/policy-gradient/</guid><description>
<![CDATA[<h1>策略梯度定理</h1><p>作者：GlowLED（zpeiyu11@gamil.com）</p>
        
          <h2 id="policy-based算法">
<a class="header-anchor" href="#policy-based%e7%ae%97%e6%b3%95"></a>
Policy-Based算法
</h2><p>Value-Based算法试图学习$Q$函数, 通过评估每个动作得到回报的期望来获得一个理性的策略$\pi$. Policy-Based则直接学习策略$\pi$.</p>
<p>因此, 我们先要显式定义$\pi$, 将其参数化为$\pi_\theta$. $\pi_\theta(s)$是一个分布$\mathbb{P}(\cdot|s; \theta)$. 而优化$\theta$可以使用梯度上升法. 即策略梯度方法.</p>
<h2 id="策略梯度方法">
<a class="header-anchor" href="#%e7%ad%96%e7%95%a5%e6%a2%af%e5%ba%a6%e6%96%b9%e6%b3%95"></a>
策略梯度方法
</h2><p>首先我们需要设计一个用于梯度上升的目标函数$ J(\theta) $. 直觉上来说, 我们希望策略获得的奖励期望最大, 即:
</p>
$$
\begin{align*}
J(\theta) = \mathbb{E}_\tau[R(\tau)]	\tag{1}
\end{align*}
$$<p>
$ \tau $即轨迹(trajectory), 该式即对所有可能的轨迹, 求奖励的期望. 这个式子虽然没写$\theta$, 但是其实和$\theta$有关. 因为:
</p>
$$
\begin{align*}
\mathbb{E_\tau}[R(t)] = \sum_\tau P(\tau ; \theta)R(\tau)	\tag{2}
\end{align*}
$$<p>
$\theta $可以影响轨迹$ \tau $的分布. 因此来影响期望. 接下来, 我们对目标函数求梯度:
</p>
$$
\begin{align*}
\nabla_\theta J(\theta) &= \nabla_\theta \sum_\tau P(\tau ; \theta)R(\tau) 	\tag{3.a}\\
&= \sum_\tau R(\tau) \nabla_\theta P(\tau ; \theta)	\tag{3.b}
\end{align*}
$$<p>
如果我们可以通过计算机计算或者估计这个梯度值, 我们就可以更新我们的策略参数.</p>
        
        <hr><p>本文2026-02-03首发于<a href='https://blog.glowled.top/'>GlowLED的后花园</a>，最后修改于2026-02-03</p>]]></description><category>强化学习</category><category>随笔</category></item><item><title>Deep Q Network (DQN)笔记</title><link>https://blog.glowled.top/post/deep-q-network/</link><pubDate>Thu, 29 Jan 2026 17:10:14 +0800</pubDate><author>zpeiyu11@gamil.com (GlowLED)</author><guid>https://blog.glowled.top/post/deep-q-network/</guid><description>
<![CDATA[<h1>Deep Q Network (DQN)笔记</h1><p>作者：GlowLED（zpeiyu11@gamil.com）</p>
        
          <h2 id="基本方法">
<a class="header-anchor" href="#%e5%9f%ba%e6%9c%ac%e6%96%b9%e6%b3%95"></a>
基本方法
</h2><p><strong>Deep Q Network</strong>的核心思想是使用神经网络表示:
</p>
$$
Q_\pi(s_t, a_t) = Net_\theta(s_t, a_t)
$$<p>
其中, 不同的参数$ \theta $就对应了不同的$ \pi $. 策略$ \pi $被隐式表达了. 假设我们使得网络学习到了最好的参数$ \theta^* $. 则它应该满足:
</p>
$$
Q^*(s_t, a_t) = Net_{\theta^*}(s_t, a_t)
$$<p>
$ Q^* $是<strong>Optimal Action-Value Function</strong>, 最优行动价值函数, 它的值绝对是正确的, 即它的值一定是回报的期望.</p>
<p>对于Agent来说, 它采取动作的方法是:
</p>
$$
a^* = \mathop{\arg\max}\limits_{a}\ Net_{\theta^*}(s_t, a)
$$<p>
一般来说, 神经网络的实现是同时输出所有动作的$ Q $值:
</p>
$$
Net_\theta(s_t) = 
\begin{bmatrix}
Q_\pi(s_t, a_1) \\
Q_\pi(s_t, a_2) \\
Q_\pi(s_t, a_3) \\
\vdots \\
Q_\pi(s_t, a_n)
\end{bmatrix}
$$<p>
这样效率更高, 也方便更泛化地学习.</p>
        
        <hr><p>本文2026-01-29首发于<a href='https://blog.glowled.top/'>GlowLED的后花园</a>，最后修改于2026-01-29</p>]]></description><category>深度学习</category><category>强化学习</category><category>随笔</category></item><item><title>强化学习基础-粗略过一遍</title><link>https://blog.glowled.top/post/reinforcement-learning-base-quickpass/</link><pubDate>Wed, 28 Jan 2026 14:42:14 +0800</pubDate><author>zpeiyu11@gamil.com (GlowLED)</author><guid>https://blog.glowled.top/post/reinforcement-learning-base-quickpass/</guid><description>
<![CDATA[<h1>强化学习基础-粗略过一遍</h1><p>作者：GlowLED（zpeiyu11@gamil.com）</p>
        
          <h2 id="基本概念">
<a class="header-anchor" href="#%e5%9f%ba%e6%9c%ac%e6%a6%82%e5%bf%b5"></a>
基本概念
</h2><h3 id="场景描述">
<a class="header-anchor" href="#%e5%9c%ba%e6%99%af%e6%8f%8f%e8%bf%b0"></a>
场景描述
</h3><ul>
<li>
<p>Environment: 环境. 是一个外部系统, 智能体处于这个系统中，能够感知到这个系统并且能够基于感知到的状态做出一定的行动。</p>
</li>
<li>
<p>state/observation: 状态/观测. 状态反映了世界的全部信息, 观测是状态的子集.</p>
</li>
<li>
<p>agent: 智能体. 做出决策的个体.</p>
</li>
<li>
<p>action: 动作. 不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间(action space)，包括离散动作空间(discrete action spaces)和连续动作空间(continuous action spaces).</p>
</li>
<li>
<p>reward: 奖励. 是由环境给的一个标量的反馈信号(scalar feedback signal).</p>
</li>
</ul>
<p>强化学习设想中, agent在environment中行动, 在某时刻获取observation, 采取action并促使environment做出改变. 在某些时刻可以获取reward. 强化学习的目标是使得reward总和最大化.</p>
<h3 id="公理化">
<a class="header-anchor" href="#%e5%85%ac%e7%90%86%e5%8c%96"></a>
公理化
</h3><p>动作使用$ a $表示, 状态(观测)使用$ s $表示, 时刻使用$ t $表示.</p>
<ol>
<li>
<p><strong>Policy</strong>: 策略. 用于决定下一步做出什么行动.</p>
<p>如果是确定性的, 一般用$ \mu $表示:</p>
</li>
</ol>
$$
a_t=\mu(s_t)
$$<p>​	如果是随机性的, 一般用$ \pi $表示:
</p>
$$
a_t \sim \pi(\cdot|s_t)
$$<ol start="2">
<li><strong>State Transition</strong>: 状态转移. 可以是确定的也可以是随机的. 一般认为是随机的. 可以用<strong>状态密度函数</strong>来表示:</li>
</ol>
$$
p(s'|s, a) = \mathbb{P}(S'=s'|S=s, A=a)
$$<ol start="3">
<li>
<p><strong>Return</strong>: 回报. 又称<strong>cumulated future reward</strong>, 一般表示为$ U $, 定义为:
</p>
        
        <hr><p>本文2026-01-28首发于<a href='https://blog.glowled.top/'>GlowLED的后花园</a>，最后修改于2026-01-28</p>]]></description><category>强化学习</category><category>深度学习</category></item></channel></rss>