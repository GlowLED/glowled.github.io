[{"objectID":"/post/reinforcement-learning-base-quickpass/","permalink":"/post/reinforcement-learning-base-quickpass/","title":"强化学习基础-粗略过一遍","summary":" 基本概念 场景描述 Environment: 环境. 是一个外部系统, 智能体处于这个系统中，能够感知到这个系统并且能够基于感知到的状态做出一定的行动。\nstate/observation: 状态/观测. 状态反映了世界的全部信息, 观测是状态的子集.\nagent: 智能体. 做出决策的个体.\naction: 动作. 不同的环境允许不同种类的动作，在给定的环境中，有效动作的集合经常被称为动作空间(action space)，包括离散动作空间(discrete action spaces)和连续动作空间(continuous action spaces).\nreward: 奖励. 是由环境给的一个标量的反馈信号(scalar feedback signal).\n强化学习设想中, agent在environment中行动, 在某时刻获取observation, 采取action并促使environment做出改变. 在某些时刻可以获取reward. 强化学习的目标是使得reward总和最大化.\n公理化 动作使用$ a $表示, 状态(观测)使用$ s $ …\n","date":"2026-01-28","categories":["强化学习","深度学习"],"tags":["技术"]},{"objectID":"/post/volume-rendering/","permalink":"/post/volume-rendering/","title":"体渲染-随笔","summary":" 本篇是随笔, 虽然写了很久, 但是乱写\n写了我好久, 感觉高数都还回去了.\n寒假是真的无聊\u0026hellip;在22: 27看高数的绝望\u0026hellip;\n引入 体渲染是一种渲染模型, 也可以认为是一种渲染思路. 在Mesh的渲染中, 我们认为场景其实是面片+材质, 而体渲染认为物体应当是个有体积的东西. 光穿过物体, 在这个过程中不断与物体发生作用, 最后得到结果.\n体渲染的引入, 一开始是为了解决云, 烟雾这类物体的渲染. 可以认为, 面片渲染只考虑了光与物体表面的作用, 没有考虑光在物体内部的行为. 这在大部分物体上可能没有什么问题, 例如玻璃与金属, 几乎所有光学行为都发生在表面. 但是当问题复杂一些: 我想要体现水中的丁达尔效应, 我想要渲染逼真的云雾, 面片模型就难以胜任. 一种解决方法是将丁达尔效应建模成简单的射线, 再套用一个衰减函数. 但有时候云雾并不完全是这样的, 光也会在内部散射, 甚至粒子有自发光, 云雾并不均匀. 于是, 我们不如考虑光在场景中的整个行进过程, 而不是只考虑表面这种割裂情形. 这就是体渲染(Volume Rendering).\n体渲染 …","date":"2026-01-27","categories":["随笔"],"tags":["计算机图形学","技术"]},{"objectID":"/post/differentiable-rendering/","permalink":"/post/differentiable-rendering/","title":"可微渲染-随笔","summary":" 本篇是随笔, 也就是随便写写一些想法, 这些想法内容不足以成文, 因此也并不考究.\n我之前只接触过传统CG, 不如说传统CG都没怎么学明白. 实际上基本只学了Path Tracing, 确实是简单又出效果, 但是这导致我图形学数学基础非常烂. 最近由于要研究3DGS, 一大堆术语都是听过但是不理解. 比如球谐函数, 可微渲染种种. 我觉得不写出来就学得不踏实\u0026hellip;\u0026hellip;\n所以会更一些图形学相关的下饭小文章, 纯粹是自己的输出, 没有太在乎可读性. 不过我是比较难接受抽象想法的人, 只有把事情想得直观一些才能理解。所以应该(?)会比较简单.\n试想一下我们的计算机图形学渲染过程是啥样的: $$ 场景参数 + 算法 \\rightarrow 图像 $$ 嗯, 算法好理解, 也就是我们整的那些渲染管线, 一般来说就是些坐标变换, 最后有个光栅化操作.\n场景参数是啥? 实际上就是你对场景的控制, 一些可设定的值. 例如, 当场景中只能有一个长方体时, 你的场景参数或许就是长方体顶点坐标, 长方体旋转角度\u0026hellip;\u0026hellip;总之, 这些参数唯一确定了场景中的 …\n","date":"2026-01-26","categories":["随笔"],"tags":["计算机图形学","技术"]},{"objectID":"/post/gqa-group-query-attention-notes-and-implementation/","permalink":"/post/gqa-group-query-attention-notes-and-implementation/","title":"GQA分组查询注意力学习笔记与代码实现","summary":" 上一篇文章搞定了RoPE，接下来该实现llama中使用的另外一个关键技术：Group Query Attention，分组查询注意力机制。 相比RoPE，个人觉得这个好理解一些，不过代码实现上难度大一些。当然了，我还是给它实现出来了。不过有关解码的部分，GQA的kv cache与原来的MHA不太一样，目前我的项目还没做到解码的部分，等我的SimpleLLM项目实现到这一部分应该会在后面加上，或者专门写一篇文章讲讲解码的全过程实现，里面会包含这几种注意力机制（还有MQA）的解码。\n手搓LLM架构与训练过程：SimpleLLM\n在讲解GQA之前，我们先讲讲最开始的MHA（Multi Head Attention，多头注意力机制），与改进后的MQA（Multi Query Attention，多查询注意力机制）。\nMHA与MQA 先讲MHA，多头注意力机制，这是在transformer中最开始的注意力机制，相信大家也比较熟悉。\n我们现在有这么一个序列： $$ Sequence=\\{token_i\\}_{i=1}^N $$ 经过embedding之后，得到的词 …\n","date":"2025-07-05","categories":["深度学习"],"tags":["手搓大模型计划","技术"]},{"objectID":"/post/rope-positional-encoding-notes-and-implementation/","permalink":"/post/rope-positional-encoding-notes-and-implementation/","title":"RoPE旋转位置编码学习笔记与代码实现","summary":" 最近想写点torch，就打算手搓个llm预训练，于是就去拿llama3开刀。尝试去实现久闻大名的RoPE（旋转位置编码）的时候，搜到了这篇文章，然后就直接开看。奈何本人数学水平实在太差，横竖看不进去，总是不太能理解。于是就干脆对着公式写代码，经过一段痛苦的时间后，勉强挤出几十行代码，才算是对RoPE有了一些不严谨的理解。\n最近心血来潮整的手搓llm结构与训练过程：SimpleLLM (应该会摆掉，应该\u0026hellip;)\n绝对位置编码的缺陷 绝对位置编码是这么做的： $$ x'_m = f(x_m,m) $$ 是第m个位置的token进行词嵌入后得到的向量， 则是我们想要得到的，附加了位置信息的第m个位置的token对应的初始语义向量。原论文的绝对位置编码通过直接更改输入到模型内部的词嵌入向量的值，来实现附加位置信息。\n这种方法非常符合直觉，在输入进模型之前进行某种意义上的“标号”，来附加位置信息。只要每个位置附加的信息都是独一无二的，那么它就可以作为这个位置的唯一标识。散落无章的一堆纸，只要有了页码，就可以排成一本书。\n但是这个方法也有问题。至于是什么问题呢？本人对此有一些不 …\n","date":"2025-07-02","categories":["深度学习"],"tags":["手搓大模型计划","技术"]}]