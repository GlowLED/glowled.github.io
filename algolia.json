[{"objectID":"/post/gqa-group-query-attention-notes-and-implementation/","permalink":"/post/gqa-group-query-attention-notes-and-implementation/","title":"GQA分组查询注意力学习笔记与代码实现","summary":" 上一篇文章搞定了RoPE，接下来该实现llama中使用的另外一个关键技术：Group Query Attention，分组查询注意力机制。 相比RoPE，个人觉得这个好理解一些，不过代码实现上难度大一些。当然了，我还是给它实现出来了。不过有关解码的部分，GQA的kv cache与原来的MHA不太一样，目前我的项目还没做到解码的部分，等我的SimpleLLM项目实现到这一部分应该会在后面加上，或者专门写一篇文章讲讲解码的全过程实现，里面会包含这几种注意力机制（还有MQA）的解码。\n手搓LLM架构与训练过程：SimpleLLM\n在讲解GQA之前，我们先讲讲最开始的MHA（Multi Head Attention，多头注意力机制），与改进后的MQA（Multi Query Attention，多查询注意力机制）。\nMHA与MQA 先讲MHA，多头注意力机制，这是在transformer中最开始的注意力机制，相信大家也比较熟悉。\n我们现在有这么一个序列： $$ Sequence=\\{token_i\\}_{i=1}^N $$ 经过embedding之后，得到的词嵌 …\n","date":"2025-07-05","categories":["深度学习"],"tags":["手搓大模型计划","技术"]},{"objectID":"/post/rope-positional-encoding-notes-and-implementation/","permalink":"/post/rope-positional-encoding-notes-and-implementation/","title":"RoPE旋转位置编码学习笔记与代码实现","summary":" 最近想写点torch，就打算手搓个llm预训练，于是就去拿llama3开刀。尝试去实现久闻大名的RoPE（旋转位置编码）的时候，搜到了这篇文章，然后就直接开看。奈何本人数学水平实在太差，横竖看不进去，总是不太能理解。于是就干脆对着公式写代码，经过一段痛苦的时间后，勉强挤出几十行代码，才算是对RoPE有了一些不严谨的理解。\n最近心血来潮整的手搓llm结构与训练过程：SimpleLLM (应该会摆掉，应该\u0026hellip;)\n绝对位置编码的缺陷 绝对位置编码是这么做的： $$ x'_m = f(x_m,m) $$ 是第m个位置的token进行词嵌入后得到的向量， 则是我们想要得到的，附加了位置信息的第m个位置的token对应的初始语义向量。原论文的绝对位置编码通过直接更改输入到模型内部的词嵌入向量的值，来实现附加位置信息。\n这种方法非常符合直觉，在输入进模型之前进行某种意义上的“标号”，来附加位置信息。只要每个位置附加的信息都是独一无二的，那么它就可以作为这个位置的唯一标识。散落无章的一堆纸，只要有了页码，就可以排成一本书。\n但是这个方法也有问题。至于是什么问题呢？本人对此有一些不成 …\n","date":"2025-07-02","categories":["深度学习"],"tags":["手搓大模型计划","技术"]},{"objectID":"/post/test/","permalink":"/post/test/","title":"Test","summary":"Test\n这是一篇测试文章，用来验证Hugo博客系统是否正常工作。\n","date":"2025-07-02","categories":["测试"],"tags":["Hugo","Test"]},{"objectID":"/post/test-copy/","permalink":"/post/test-copy/","title":"Test copy","summary":"Test\n这是一篇测试文章，用来验证Hugo博客系统是否正常工作。 11111 11111\n","date":"2025-07-02","categories":["测试"],"tags":["Hugo","Test"]}]