[{"objectID":"1751433774","permalink":"/post/rope%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/","title":"RoPE旋转位置编码学习笔记与代码实现","content":" 最近想写点torch，就打算手搓个llm预训练，于是就去拿llama3开刀。尝试去实现久闻大名的RoPE（旋转位置编码）的时候，搜到了这篇文章，然后就直接开看。奈何本人数学水平实在太差，横竖看不进去，总是不太能理解。于是就干脆对着公式写代码，经过一段痛苦的时间后，勉强挤出几十行代码，才算是对RoPE有了一些不严谨的理解。\n最近心血来潮整的手搓llm结构与训练过程：SimpleLLM (应该会摆掉，应该\u0026amp;hellip;)\n绝对位置编码的缺陷 绝对位置编码是这么做的：  是第m个位置的token进行词嵌入后得到的向量， 则是我们想要得到的，附加了位置信息的第m个位置的token对应的初始语义向量。原论文的绝对位置编码通过直接更改输入到模型内部的词嵌入向量的值，来实现附加位置信息。\n这种方法非常符合直觉，在输入进模型之前进行某种意义上的“标号”，来附加位置信息。只要每个位置附加的信息都是独一无二的，那么它就可以作为这个位置的唯一标识。散落无章的一堆纸，只要有了页码，就可以排成一本书。\n但是这个方法也有问题。至于是什么问题呢？本人对此有一些不成熟的思考：\n我们先从llama3的训练中的一个细节出发：Attention is All You Need这篇论文中模型的训练，与BERT的训练，从它们的数据集中拿出某一个sample，那大概率是填不满上下文长度的，于是需要对上下文进行padding来达成固定长度。假设上下文长度为16K，甲sample的序列长度为1K，乙sample的序列长度为8K，两者都填充到了16K，开销上来看是差不多的，因为数据的规模一样大，都是一样计算（猜测可能内部有一些优化，但是不会差太多），但是从直觉上来说，应该是乙sample包含的有效信息更多，更加利于模型优化。\n于是，很自然的，当模型上下文长度比较大，没多少sample能填满时，一个输入的序列应该由多个sample拼接而成，凑成一个比较接近上下文长度的序列，充分利用算力。当然了，为了防止某一个sample中的token进行注意力计算时，去查询了其它sample的token，mask也要进行一些改造。不过本篇文章的重点不是这个。\n在这种情况下，我们其实希望位置编码的信息要有某种“等价性”：无论其中一个sample在哪个位置，位置编码信息应该不影响位置的表达。一个sample在拼接过程中被排在第一个，中间的位置，还是最后的位置，位置编码信息都应该具有某种共通的性质，因为这句话无论排在哪都是同一句话，位置编码出的向量都应该包含相同的信息。\n这种特征的“空间平移不变性”，让人联想到CNN对于特征位置的不敏感。原因是CNN天生擅长捕捉像素相对位置的信息，毕竟本身结构的设计就是为了考虑相对位置。\n所以要实现这种对于空间的绝对位置不敏感，无论在序列中的哪个位置都能表达相同的模式的编码方式，考虑相对位置信息很重要。\n这么来看，原来的绝对位置编码的方式其实就有点不太合适了。为什么呢？我们观察一下原来绝对位置编码使用的Sinusoidal 函数：  可以看到，Sinusoidal 函数基本不强调“相对位置信息”，不同的位置，甚至连变化的feature都不是同一个，例如，在position大约为0-5时，相邻位置编码信息在大于40的feature上几乎无变化。而当position增大后，feature较大的位置也发生了变化。位置之间的差距甚至都不是同一个feature，都不是一个方面的比较。\n绝对位置编码错开了不同位置间差距的feature的位置，虽然充分利用了语义向量中的每个feature，但是也基本毁灭了相对位置信息的一致性。这使得同一个sample被拼接到开头，中间与结尾的位置信息都不太一样，模型需要花费额外的参数来去学习这种没有规律的不同。仿佛就是同样一句话只是因为开头被放在了不同位置，token之间的位置关系就变了。\n现在我们放远视角，我们不仅关心llama3训练这种一输入多样本的情况。考虑大多数文章。同样一句话在文章的开头与结尾虽然有不同的绝对位置信息，但也应该具有相同的相对位置信息，而不是相对位置信息有很大差别。\n为了解决这个问题，我们人为地设计一种可以在绝对位置信息的基础上表达相对位置信息的规律，这样模型能很容易地学习到这个规律。这就有了RoPE。而RoPE选择的规律是角度。\nRoPE，旋转位置编码 与绝对位置编码不同，RoPE是在q，k，v生成之后，对q与k做手脚，来附加位置信息。附加完后，q，k再正常计算attn score，然后对v进行加权和（attention的正常流程）。\n本人数学水平堪忧，中间过程怕讲错，所以这里直接看结果的几个式子：  上面这个式子就是我们想要的这样一个编码函数的表达式：它接受输入 (第m个token进行词嵌入后得到的向量)，以及位 …","summary":" 最近想写点torch，就打算手搓个llm预训练，于是就去拿llama3开刀。尝试去实现久闻大名的RoPE（旋转位置编码）的时候，搜到了这篇文章，然后就直接开看。奈何本人数学水平实在太差，横竖看不进去，总是不太能理解。于是就干脆对着公式写代码，经过一段痛苦的时间后，勉强挤出几十行代码，才算是对RoPE有了一些不严谨的理解。\n最近心血来潮整的手搓llm结构与训练过程：SimpleLLM (应该会摆 …\n","date":"2025-07-02"},{"objectID":"1751421384","permalink":"/post/test/","title":"Test","content":"Test\n这是一篇测试文章，用来验证Hugo博客系统是否正常工作。\n测试内容 这里是文章的详细内容。\n功能测试 文章显示 分类和标签 评论功能 搜索功能 代码示例 def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) return \u0026#34;成功发布文章\u0026#34; hello_world() 这是文章的结尾。\n ","summary":"Test\n这是一篇测试文章，用来验证Hugo博客系统是否正常工作。\n","date":"2025-07-02"},{"objectID":"1751421384","permalink":"/post/test-copy/","title":"Test copy","content":"Test\n这是一篇测试文章，用来验证Hugo博客系统是否正常工作。 11111 11111\n测试内容 这里是文章的详细内容。\n功能测试 文章显示 分类和标签 评论功能 搜索功能 代码示例 def hello_world(): print(\u0026#34;Hello, Hugo!\u0026#34;) return \u0026#34;成功发布文章\u0026#34; hello_world() 这是文章的结尾。\n","summary":"Test\n这是一篇测试文章，用来验证Hugo博客系统是否正常工作。 11111 11111\n","date":"2025-07-02"}]